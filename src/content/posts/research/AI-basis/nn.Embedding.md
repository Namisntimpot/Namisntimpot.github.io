---
title: nn.Embedding
published: 2023-07-15
description: 偶尔记录一些零碎的小知识...nn.Embedding. 所谓“嵌入”。
tags: [research, 深度学习基础]
category: 科研
draft: false
---

有时候看到的所谓“结构化潜码”，即给固定数量的某物每个分配一个编码，就是这个nn.Embedding。  
他本来用在语言处理中比较多，对一个固定大小的字典，对字典中每个元素分配一个*查找向量*，能够直接从这个向量，根据字典，得知这是个什么字（元素）。  
显然，字典和字可以扩展到很多概念使用。  
  
最简单的想法就是one-hot编码：字典有N个元素，就用一个N维向量表示其中任何一个元素。第 i 个元素的编码就是把第 i 位置1，其它置0.  
但这显然太低效。所以有了这个方法来用低维度的向量唯一表示字典中任一元素。  
```  
nn.Embedding(num_words, embedding_dim)  
```  
num_words就是字典大小，这个字典中有几个元素。embedding_dim表示要用多长的向量表示每个元素。  
  
用矩阵乘法的角度考虑，可以将 embedding 的过程拆为两步：首先用 one-hot 码表示句子中的每个字，然后乘以一个矩阵降维为低维向量。1 x N \* N x embedding_dim. 显然，如果要表示的句子（字的有序集合）长度不为1，就不是1.  
  
前面的one-hot编码是固定的，后面的权重（第二个矩阵）是可以学习的。不过理论上应该要保证这么乘出来的结果每个元素都唯一，暂不知如何进行这个约束。  
